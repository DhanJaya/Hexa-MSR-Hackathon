import csv
import json
import os
import shutil
import datetime

from graal.backends.core.covuln import CoVuln


def read_csv_file():
    with open('../data/prdata.csv') as pr_data:
        csv_reader = csv.reader(pr_data, delimiter=',')
        next(csv_reader)
        for row in csv_reader:
            run_graal_analysis(row[0], row[1], row[2], row[3], row[4], row[5])


def remove_directories(directory):
    if os.path.exists(directory):
        shutil.rmtree(directory)
        print('Removed: ' + directory)
    if os.path.exists('/tmp/worktrees'):
        shutil.rmtree('/tmp/worktrees')
        print('Removed: /tmp/worktrees')


def run_graal_analysis(repo_url, pr, fork_url, from_date, to_date, commit_hashes):
    repo_details = fork_url.split('/')[-2:]
    owner = repo_details[0]
    repo = repo_details[1]
    directory = '/tmp/' + repo
    remove_directories(directory)

    start_date_time = datetime.datetime.strptime(from_date, '%Y-%m-%dT%H:%M:%SZ') - datetime.timedelta(1)
    end_date_time = datetime.datetime.strptime(to_date, '%Y-%m-%dT%H:%M:%SZ') + datetime.timedelta(days=1)

    repo_uri = 'https://github.com/' + owner + '/' + repo
    co_vuln = CoVuln(uri=repo_uri, entrypoint='/tmp/worktrees/' + repo, git_path=directory)
    items = co_vuln.fetch(from_date=start_date_time, to_date=end_date_time)
    with open("../data/results.txt", "a") as results:
        results.write(repo_url + '/pull/' + pr + '\n' + '\t' + 'FORK: ' + fork_url + '\n' + '\tANALYSIS: \n')
    for commit in items:
        if commit['data']['commit'] in commit_hashes:
            with open("../data/results.txt", "a") as results:
                results.write('\t' + commit['data']['commit'] + ' -> ' + json.dumps(commit['data']['analysis']) + '\n')

    remove_directories(directory)


read_csv_file()
