import json
import os
import shutil
import datetime
import csv

from graal.backends.core.covuln import CoVuln

pr_file_path = '../data/prdetails/tornado-prs.csv'
text_file_path = '../data/vulnerability-new/tornado/tornado-vulnerabilities.txt'
csv_file_path = '../data/vulnerability-new/tornado/tornado-vulnerabilities.csv'
valid_pr_count = 51


def get_most_used_module_name(file_path):
    modules = {}
    file_path_list = list(eval(file_path))
    for file_path_name in file_path_list:
        if '/' in file_path_name:
            module_name = file_path_name.split('/')[0]
            if module_name not in ['test', 'tests', 'doc', 'docs']:
                if module_name in modules:
                    value = modules[module_name]
                    value += 1
                    modules[module_name] = value
                else:
                    modules[module_name] = 1
    most_used_module = None
    most_used_time = 0
    for module, number in modules.items():
        if number > most_used_time:
            most_used_module = module
            most_used_time = number
    return most_used_module


def read_csv_file():
    # Prepare CSV file to write analysis results
    if not os.path.exists(csv_file_path):
        header = ['number', 'repo', 'pr', 'fork', 'entry_point', 'from_date', 'to_date', 'commit_hash', 'commit_date',
                  'analysis']
        with open(csv_file_path, 'a', encoding='UTF8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)

    with open(pr_file_path) as pr_data:
        csv_reader = csv.reader(pr_data, delimiter=',')
        next(csv_reader)
        for row in csv_reader:
            entry_point = get_most_used_module_name(row[9])
            if row[10] == 'true' and entry_point is not None:
                run_graal_analysis(row[0], row[1], row[2], row[6], row[4], row[5], row[7], entry_point)


def remove_directories(directory):
    if os.path.exists(directory):
        shutil.rmtree(directory)
        print('Removed: ' + directory)
    if os.path.exists('/tmp/worktrees'):
        shutil.rmtree('/tmp/worktrees')
        print('Removed: /tmp/worktrees')


def run_graal_analysis(repo_url, pr, fork_url, from_date, to_date, commit_hashes, initial_commit_hash, entry_point):
    global valid_pr_count
    repo_details = fork_url.split('/')[-2:]
    owner = repo_details[0]
    repo = repo_details[1]
    directory = '/tmp/' + repo
    remove_directories(directory)

    start_date_time = datetime.datetime.strptime(from_date, '%Y-%m-%dT%H:%M:%SZ') - datetime.timedelta(2)
    end_date_time = datetime.datetime.strptime(to_date, '%Y-%m-%dT%H:%M:%SZ') + datetime.timedelta(days=2)

    repo_uri = 'https://github.com/' + owner + '/' + repo
    co_vuln = CoVuln(uri=repo_uri, entrypoint=entry_point, git_path=directory)
    items = co_vuln.fetch(from_date=start_date_time, to_date=end_date_time)
    with open(text_file_path, "a") as results:
        results.write(
            repo_url + '/pull/' + pr + '\n' + '\t' + 'FORK: ' + fork_url + '\n' + '\tPR_COUNT: ' + str(
                valid_pr_count) + '\n' + '\tENTRY_POINT: ' + entry_point + '\n' + '\tANALYSIS: ' + from_date + ' - ' + to_date + '\n')
        results.write('\t\tPR_OPEN_COMMIT: ' + initial_commit_hash + '\n')

    count = 0
    for commit in items:
        print(commit)
        with open(text_file_path, "a") as results:
            if commit['data']['commit'] in commit_hashes[
                                           commit_hashes.index(initial_commit_hash): len(commit_hashes) - 1]:
                results.write(
                    '\t\t' + commit['data'][
                        'commit'] + ' [' + commit['data']['CommitDate'] + '] -> ' + json.dumps(
                        commit['data']['analysis']) + '\n')
                data = [valid_pr_count, repo_url, pr, fork_url,entry_point, from_date, to_date, commit['data']['commit'],
                        commit['data']['CommitDate'], json.dumps(commit['data']['analysis'])]
                with open(csv_file_path, 'a', encoding='UTF8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(data)
                count += 1

    if count > 0:
        valid_pr_count += 1
    remove_directories(directory)


read_csv_file()
