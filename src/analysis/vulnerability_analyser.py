import json
import os
import shutil
import datetime
import csv

from graal.backends.core.covuln import CoVuln

pr_file_path = '../data/prs/flask-prs.csv'
text_file_path = '../data/vulnerability/flask-vulnerabilities.txt'
csv_file_path = '../data/vulnerability/flask-vulnerabilities.csv'


def read_csv_file():
    with open(pr_file_path) as pr_data:
        csv_reader = csv.reader(pr_data, delimiter=',')
        next(csv_reader)
        header = ['repo', 'pr', 'fork', 'from_date', 'to_date', 'commit_hash', 'commit_date',
                  'analysis']
        with open(csv_file_path, 'a', encoding='UTF8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
        for row in csv_reader:
            run_graal_analysis(row[0], row[1], row[2], row[6], row[4], row[5], row[7])


def remove_directories(directory):
    if os.path.exists(directory):
        shutil.rmtree(directory)
        print('Removed: ' + directory)
    if os.path.exists('/tmp/worktrees'):
        shutil.rmtree('/tmp/worktrees')
        print('Removed: /tmp/worktrees')


def run_graal_analysis(repo_url, pr, fork_url, from_date, to_date, commit_hashes, initial_commit_hash):
    repo_details = fork_url.split('/')[-2:]
    owner = repo_details[0]
    repo = repo_details[1]
    directory = '/tmp/' + repo
    remove_directories(directory)

    start_date_time = datetime.datetime.strptime(from_date, '%Y-%m-%dT%H:%M:%SZ') - datetime.timedelta(2)
    end_date_time = datetime.datetime.strptime(to_date, '%Y-%m-%dT%H:%M:%SZ') + datetime.timedelta(days=2)

    repo_uri = 'https://github.com/' + owner + '/' + repo
    co_vuln = CoVuln(uri=repo_uri, entrypoint='/tmp/worktrees/' + repo, git_path=directory)
    items = co_vuln.fetch(from_date=start_date_time, to_date=end_date_time)
    with open(text_file_path, "a") as results:
        results.write(
            repo_url + '/pull/' + pr + '\n' + '\t' + 'FORK: ' + fork_url + '\n' + '\tANALYSIS: ' + from_date + ' - ' + to_date + '\n')
        results.write('\t\tPR_OPEN_COMMIT: ' + initial_commit_hash + '\n')

    count = 0
    for commit in items:
        with open(text_file_path, "a") as results:
            if commit['data']['commit'] in commit_hashes[
                                           commit_hashes.index(initial_commit_hash): len(commit_hashes) - 1]:
                results.write(
                    '\t\t' + commit['data'][
                        'commit'] + ' [' + commit['data']['CommitDate'] + '] -> ' + json.dumps(
                        commit['data']['analysis']) + '\n')
                data = [repo_url, pr, fork_url, from_date, to_date, commit['data']['commit'],
                        commit['data']['CommitDate'], json.dumps(commit['data']['analysis'])]
                with open(csv_file_path, 'a', encoding='UTF8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(data)
                count += 1
    remove_directories(directory)


read_csv_file()
